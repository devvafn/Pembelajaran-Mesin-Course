{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UAS Paper 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcs5d8+TTSBy9m7tzJ5Pxi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likunglaif/Pembelajaran-Mesin-Course/blob/main/UAS/ExamPaper-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qOTUjx4XsvL",
        "outputId": "73c012ec-bdf2-4b49-81d0-0019659298a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 300000/300000\n",
            "Saved processed tweets to: csv/test-processed-processed.csv\n"
          ]
        }
      ],
      "source": [
        "!python2 code/preprocess.py csv/test.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/preprocess.py csv/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsP9KjAQc27J",
        "outputId": "90711ab2-70fb-43de-aa0b-59e0fbe9ee69"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: csv/train-processed-processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/stats.py csv/train-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pawFG1uMdvFD",
        "outputId": "897fa399-f35c-4fc7-c0bb-127238e74ef3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to csv/train-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to csv/train-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 88958, Avg: 0.8896, Max: 12\n",
            "URLs => Total: 3602, Avg: 0.0360, Max: 4\n",
            "Emojis => Total: 633, Positive: 442, Negative: 191, Avg: 0.0063, Max: 3\n",
            "Words => Total: 1190790, Unique: 50335, Avg: 11.9079, Max: 40, Min: 0\n",
            "Bigrams => Total: 1091450, Unique: 384499, Avg: 10.9145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/baseline.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcmDyiiyeble",
        "outputId": "37651668-b826-46fe-d686-a19d5fa74b6a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 65.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/naivebayes.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ7ZNMaoe9CU",
        "outputId": "4a6114fa-6a09-43fa-c2d1-c16c2891eabc"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7790/10000 = 77.9000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/maxent-nltk.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX-wULXdpllU",
        "outputId": "2b148b46-6a84-411d-8702-84639f68d20a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (1 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.565\n",
            "         Final          -0.61588        0.755\n",
            "  -1.112 shaundiviney==True and label is '0'\n",
            "   1.000 thestreetforce==True and label is '1'\n",
            "   1.000 housee==True and label is '1'\n",
            "   1.000 mcflyforgermany==True and label is '1'\n",
            "   1.000 hitech==True and label is '1'\n",
            "   1.000 justin.tv==True and label is '0'\n",
            "   1.000 brazilwillmissmcfly==True and label is '0'\n",
            "   1.000 madnessness==True and label is '0'\n",
            "   1.000 notfeelingwell==True and label is '0'\n",
            "   1.000 halfbloodprince==True and label is '1'\n",
            "Validation set accuracy:0.0000\n",
            "\n",
            "Predicting for test data\n",
            "\n",
            "Saved to maxent.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/decisiontree.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H7QzIDxq-2X",
        "outputId": "d376356d-7e7b-45ae-c901-dd9bddc7bdf1"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6829/10000 = 68.2900 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/randomforest.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iH6FcorrP7Q",
        "outputId": "613a0d20-be64-4f58-8322-cc994f8b6b30"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7226/10000 = 72.2600 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/svm.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJuPD0Sn9wrV",
        "outputId": "6ab8acc4-7ffc-41f3-ceae-8c15032d3d2f"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7852/10000 = 78.5200 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install utils\n",
        "!pip3 install tensorflow-gpu\n",
        "!pip3 install tensorflow==1.15\n",
        "!pip3 install keras==2.3.1\n",
        "!pip3 install h5py==2.10.0"
      ],
      "metadata": {
        "id": "-MAEmf99DYi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "import random\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = 'csv/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = 'csv/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = 'csv/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = 'csv/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 1500\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 100\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            utils.write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(20)\n",
        "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = utils.top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = utils.split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=20)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        utils.write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            utils.write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            utils.write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        utils.save_results_to_csv(predictions, 'xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhfddVO6Bv2b",
        "outputId": "cd1def29-3839-4a08-8adf-be9360d33df3"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7179/10000 = 71.7900 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python code/neuralnet.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KXgM53O-ArA",
        "outputId": "65b97521-0d24-47c1-eee9-b8692b0bc570"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2022-01-21 20:50:00.780566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-01-21 20:50:00.810462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:50:00.811040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-21 20:50:00.811395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-21 20:50:00.812783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-21 20:50:00.814033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-21 20:50:00.814416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-21 20:50:00.816369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-21 20:50:00.817656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-21 20:50:00.824737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-21 20:50:00.824854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:50:00.825480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:50:00.825998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-21 20:50:00.826332: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2022-01-21 20:50:00.834061: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000144999 Hz\n",
            "2022-01-21 20:50:00.834263: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55aa42214d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-21 20:50:00.834292: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-01-21 20:50:00.935707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:50:00.936444: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55aa42214bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-21 20:50:00.936478: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2022-01-21 20:50:00.936655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:50:00.937220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-21 20:50:00.937298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-21 20:50:00.937324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-21 20:50:00.937346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-21 20:50:00.937366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-21 20:50:00.937390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-21 20:50:00.937410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-21 20:50:00.937432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-21 20:50:00.937508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:50:00.938089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:50:00.938595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-21 20:50:00.938664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-21 20:50:00.940115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-01-21 20:50:00.940147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2022-01-21 20:50:00.940163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2022-01-21 20:50:00.940341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:50:00.940931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:50:00.941506: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-01-21 20:50:00.941549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "2022-01-21 20:50:01.709105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "Iteration 180/180, loss:0.4851, acc:0.7880\n",
            "Epoch: 1, val_acc:0.7609\n",
            "Accuracy improved from 0.0000 to 0.7609, saving model\n",
            "Iteration 180/180, loss:0.4683, acc:0.7640\n",
            "Epoch: 2, val_acc:0.7683\n",
            "Accuracy improved from 0.7609 to 0.7683, saving model\n",
            "Iteration 180/180, loss:0.4571, acc:0.7820\n",
            "Epoch: 3, val_acc:0.7673\n",
            "Iteration 180/180, loss:0.4388, acc:0.7940\n",
            "Epoch: 4, val_acc:0.7646\n",
            "Iteration 180/180, loss:0.4533, acc:0.7940\n",
            "Epoch: 5, val_acc:0.7634\n",
            "Testing\n",
            "Generating feature vectors\n",
            "Processing 300000/300000\n",
            "\n",
            "Predicting batches\n",
            "Processing 600/600\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfGhTGAzBFO1",
        "outputId": "509c1f2f-f2e8-45eb-c657-3afb6551f8a4"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-21 19:02:04--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-01-21 19:02:04--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-01-21 19:02:05--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.12MB/s    in 2m 41s  \n",
            "\n",
            "2022-01-21 19:04:47 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Gvr3vHBH_H",
        "outputId": "ddc62b24-4b23-4fa7-884f-55bd7a1f7a8f"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/lstm.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYaN1-5LAoNw",
        "outputId": "4b1dcf04-3c59-44c2-b8a7-da2fac693d89"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 65.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python code/cnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMRpCOfrH3Pm",
        "outputId": "2ede68a0-9341-4121-cef7-e638047b35b9"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "Looking for GLOVE seeds\n",
            "Processing 400000/0\n",
            "\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "2022-01-21 20:58:55.844480: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-01-21 20:58:55.876522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:58:55.877121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-21 20:58:55.877422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-21 20:58:55.878661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-21 20:58:55.879852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-21 20:58:55.880212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-21 20:58:55.881577: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-21 20:58:55.882631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-21 20:58:55.886016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-21 20:58:55.886145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:58:55.886745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:58:55.887286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-21 20:58:55.887603: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2022-01-21 20:58:55.891643: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000144999 Hz\n",
            "2022-01-21 20:58:55.892068: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a9cb24ebc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-21 20:58:55.892099: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-01-21 20:58:55.988955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:58:55.989665: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a9cb24ed80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-21 20:58:55.989698: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2022-01-21 20:58:55.989880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:58:55.990473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-21 20:58:55.990543: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-21 20:58:55.990572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-21 20:58:55.990593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-21 20:58:55.990617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-21 20:58:55.990637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-21 20:58:55.990656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-21 20:58:55.990676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-21 20:58:55.990750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:58:55.991371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:58:55.991899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-21 20:58:55.991972: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-21 20:58:55.993172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-01-21 20:58:55.993201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2022-01-21 20:58:55.993212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2022-01-21 20:58:55.993334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:58:55.993918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-21 20:58:55.994453: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-01-21 20:58:55.994495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "2022-01-21 20:58:58.166848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-21 20:58:58.362906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "90000/90000 [==============================] - 21s 234us/step - loss: 0.5366 - accuracy: 0.7210 - val_loss: 0.4773 - val_accuracy: 0.7703\n",
            "Epoch 2/3\n",
            "90000/90000 [==============================] - 18s 202us/step - loss: 0.4527 - accuracy: 0.7851 - val_loss: 0.4570 - val_accuracy: 0.7828\n",
            "Epoch 3/3\n",
            "90000/90000 [==============================] - 18s 202us/step - loss: 0.4019 - accuracy: 0.8166 - val_loss: 0.4730 - val_accuracy: 0.7794\n"
          ]
        }
      ]
    }
  ]
}